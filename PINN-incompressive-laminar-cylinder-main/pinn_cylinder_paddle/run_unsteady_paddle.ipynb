{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. \n",
    "# This directory will be recovered automatically after resetting environment. \n",
    "!ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.\n",
    "# View personal work directory. \n",
    "# All changes under this directory will be kept even after reset. \n",
    "# Please clean unnecessary files in time to speed up environment loading. \n",
    "!ls /home/aistudio/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-07-27T22:25:04.495245Z",
     "iopub.status.busy": "2022-07-27T22:25:04.494231Z",
     "iopub.status.idle": "2022-07-27T22:25:05.276611Z",
     "shell.execute_reply": "2022-07-27T22:25:05.275543Z",
     "shell.execute_reply.started": "2022-07-27T22:25:04.495210Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[31mCould not open requirements file: [Errno 21] 是一个目录: '/home/aistudio/external-libraries'\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade pip\n",
    "# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:\n",
    "# If a persistence installation is required, \n",
    "# you need to use the persistence path as the following: \n",
    "# !mkdir /home/aistudio/external-libraries\n",
    "!pip uninstall matplotlib -r /home/aistudio/external-libraries\n",
    "# !pip install scikit-optimize -t /home/aistudio/external-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-28T03:28:58.170829Z",
     "iopub.status.busy": "2022-07-28T03:28:58.170060Z",
     "iopub.status.idle": "2022-07-28T03:29:01.308825Z",
     "shell.execute_reply": "2022-07-28T03:29:01.307707Z",
     "shell.execute_reply.started": "2022-07-28T03:28:58.170791Z"
    },
    "scrolled": true,
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/__init__.py:107: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import MutableMapping\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/rcsetup.py:20: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Iterable, Mapping\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/colors.py:53: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sized\n"
     ]
    }
   ],
   "source": [
    "# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可: \n",
    "# Also add the following code, \n",
    "# so that every time the environment (kernel) starts, \n",
    "# just run the following code: \n",
    "import sys \n",
    "sys.path.append('/home/aistudio/external-libraries')\n",
    "sys.path.append('/home/aistudio/work')\n",
    "sys.path.append('/home/aistudio/data')\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import time\n",
    "import os\n",
    "import scipy.io as sio\n",
    "# from basic_model import numpy_32, tensor_32\n",
    "from basic_model import PaddleModel_single, gradients\n",
    "from visual_data import matplotlib_vision\n",
    "\n",
    "os.environ['NUMEXPR_MAX_THREADS'] = '20'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-28T03:29:03.691950Z",
     "iopub.status.busy": "2022-07-28T03:29:03.691144Z",
     "iopub.status.idle": "2022-07-28T03:29:03.704063Z",
     "shell.execute_reply": "2022-07-28T03:29:03.703348Z",
     "shell.execute_reply.started": "2022-07-28T03:29:03.691915Z"
    },
    "scrolled": true,
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 输入 inn_var : x, y, t\n",
    "# 输出 out_var : p, u, v, s11, s12, s22\n",
    "class Net(PaddleModel_single):\n",
    "    def __init__(self, planes, rho, miu):\n",
    "        super(Net, self).__init__(planes, active=nn.Tanh())\n",
    "        self.rho = rho\n",
    "        self.miu = miu\n",
    "\n",
    "    # 将网络的直接输出 psi ,p, s11, s12, s22 转化为 p, u, v, s11, s12, s22\n",
    "    def output_transform(self, inn_var, out_var):\n",
    "        psi, p, s11, s22, s12 = \\\n",
    "            out_var[..., 0:1], out_var[..., 1:2], out_var[..., 2:3], out_var[..., 3:4], out_var[..., 4:5]\n",
    "\n",
    "        w = gradients(psi, inn_var)\n",
    "        u, v = w[..., 1:2], -w[..., 0:1]\n",
    "        return paddle.concat((p, u, v, s11, s12, s22), dim=-1)\n",
    "\n",
    "    # 计算残差\n",
    "    def equation(self, inn_var, out_var):\n",
    "        p, u, v, s11, s12, s22 = out_var[..., (0,)], out_var[..., (1,)], out_var[..., (2,)], \\\n",
    "                                 out_var[..., (3,)], out_var[..., (4,)], out_var[..., (5,)]\n",
    "        dpda, duda, dvda = gradients(p, inn_var), gradients(u, inn_var), gradients(v, inn_var)\n",
    "        dpdy = dpda[..., 1:2]\n",
    "        dudx, dudy, dudt, dvdx, dvdy, dvdt = duda[..., 0:1], duda[..., 1:2], duda[..., 2:3], \\\n",
    "                                             dvda[..., 0:1], dvda[..., 1:2], dvda[..., 2:3]\n",
    "\n",
    "        s11_1 = gradients(s11, inn_var)[..., 0:1]\n",
    "        s12_2 = gradients(s12, inn_var)[..., 1:2]\n",
    "        s22_2 = gradients(s22, inn_var)[..., 1:2]\n",
    "        s12_1 = gradients(s12, inn_var)[..., 0:1]\n",
    "\n",
    "        eq_p = p + (s11 + s22) / 2\n",
    "        eq_u = self.rho * dudt + self.rho * (u*dudx + v*dudy) - s11_1 - s12_2\n",
    "        eq_v = self.rho * dvdt + self.rho * (u*dvdx + v*dvdy) - s12_1 - s22_2\n",
    "        eq_s11 = -p + 2*self.miu*dudx - s11\n",
    "        eq_s22 = -p + 2*self.miu*dvdy - s22\n",
    "        eq_s12 = self.miu*(dudy+dvdx) - s12\n",
    "        eqs = paddle.concat((eq_p, eq_u, eq_v, eq_s11, eq_s12, eq_s22), dim=-1)\n",
    "\n",
    "        return eqs, paddle.concat((dpdy, dudy, dvdy), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-28T03:29:08.242244Z",
     "iopub.status.busy": "2022-07-28T03:29:08.241325Z",
     "iopub.status.idle": "2022-07-28T03:29:08.369197Z",
     "shell.execute_reply": "2022-07-28T03:29:08.368400Z",
     "shell.execute_reply.started": "2022-07-28T03:29:08.242204Z"
    },
    "scrolled": true,
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "######################## 获取 nodes 在 box 流域内的边界节点  ########################\n",
    "def BCS_ICS(nodes, box):\n",
    "    BCS = []\n",
    "    Num_Nodes = nodes.shape[0]\n",
    "    Index = np.arange(Num_Nodes)\n",
    "\n",
    "    BCS.append(Index[nodes[:, 0] == box[0]])  # inlet\n",
    "    BCS.append(Index[nodes[:, 0] == box[2]])  # outlet\n",
    "    BCS.append(Index[nodes[:, 1] == box[1]])  # top\n",
    "    BCS.append(Index[nodes[:, 1] == box[3]])  # bottom\n",
    "    BCS.append(Index[np.abs((nodes[:, 0]-0)**2 + (nodes[:, 1]-0)**2 - (D/2)**2) < 1e-7])  # cylinder wall\n",
    "\n",
    "    if nodes.shape[-1] == 3:\n",
    "        BCS.append(Index[nodes[:, 2] == 0])  # initial\n",
    "\n",
    "    return BCS\n",
    "\n",
    "\n",
    "######################## 读取数据  ########################\n",
    "def read_data(**kwargs):\n",
    "    import scipy.io as sio\n",
    "    if kwargs['steady']:\n",
    "        data = sio.loadmat('..\\\\data\\\\2D_cylinder\\\\mixed\\\\steady_data.mat')\n",
    "        INLET, OUTLET, WALL= data['INLET'][..., :2], data['OUTLET'], data['WALL']\n",
    "        num = INLET.shape[0] + OUTLET.shape[0] + WALL.shape[0]\n",
    "        XY_c  = data['XY_c'][:-num]\n",
    "        data = sio.loadmat('..\\\\data\\\\2D_cylinder\\\\mixed\\\\steady_Fluent.mat')\n",
    "        fields_fluent= np.squeeze(data['field']).T[..., (0, 1, 4, 2, 3)]\n",
    "        return XY_c, INLET, OUTLET, WALL, fields_fluent\n",
    "    else:\n",
    "        data = sio.loadmat('/home/aistudio/data/2D_cylinder/mixed/unsteady_data.mat')\n",
    "        INLET, OUTLET, WALL, INITIAL= data['INB'][..., :3], data['OUTB'], data['WALL'], data['IC']\n",
    "        num = INLET.shape[0] + OUTLET.shape[0] + WALL.shape[0]\n",
    "        XY_c  = data['XY_c'][:-num]\n",
    "    return XY_c, INLET, OUTLET, WALL, INITIAL\n",
    "\n",
    "def read_paddle_data(num_time):\n",
    "    file_path = '/home/aistudio/data/2D_cylinder/paddle_openfoam/'\n",
    "    cyl = np.loadtxt(file_path + 'domain_cylinder.csv', skiprows=1, delimiter=',')[..., (4, 5, 0, 1, 2,)] #xypuv\n",
    "    inlet = np.loadtxt(file_path + 'domain_inlet.csv', skiprows=1, delimiter=',')[..., (4, 5, 0, 1, 2,)]\n",
    "    outlet = np.loadtxt(file_path + 'domain_outlet.csv', skiprows=1, delimiter=',')[..., (4, 5, 0, 1, 2,)]\n",
    "    train = np.loadtxt(file_path + 'domain_train.csv', skiprows=1, delimiter=',')[..., (4, 5, 0, 1, 2,)]\n",
    "    initial = np.loadtxt(file_path + 'initial/ic0.1.csv', skiprows=1, delimiter=',')[..., (4, 5, 0, 1, 2,)]\n",
    "\n",
    "    # plt.figure(1)\n",
    "    # plt.plot(cyl[:, 0], cyl[:, 1], 'r.')\n",
    "    # plt.plot(inlet[:, 0], inlet[:, 1], 'b.')\n",
    "    # plt.plot(outlet[:, 0], outlet[:, 1], 'b.')\n",
    "    # plt.plot(train[:, 0], train[:, 1], 'g.')\n",
    "    # plt.show()\n",
    "    # plt.figure(2)\n",
    "    # plt.plot(train[:, 0], train[:, 1], 'k.')\n",
    "    # plt.show()\n",
    "    # plt.figure(2)\n",
    "    # plt.subplot(221)\n",
    "    # plt.scatter(initial[:, 0], initial[:, 1], s=0.1, c=initial[:, 2])\n",
    "    # plt.subplot(222)\n",
    "    # plt.scatter(initial[:, 0], initial[:, 1], s=0.1, c=initial[:, 3])\n",
    "    # plt.subplot(223)\n",
    "    # plt.scatter(initial[:, 0], initial[:, 1], s=0.1, c=initial[:, 4])\n",
    "    # plt.show()\n",
    "    # plt.figure(3)\n",
    "    # plt.plot(train[:, 0], train[:, 1], 'g.')\n",
    "    probe = []\n",
    "    times_list_all = []\n",
    "    dirs = os.listdir(file_path + 'probe/')\n",
    "    #####获取时间\n",
    "    for file in dirs:\n",
    "        time = float(file[5:-4])\n",
    "        times_list_all.append(time)\n",
    "    times_list_all = np.array(times_list_all)\n",
    "\n",
    "    times_list = np.random.choice(times_list_all, num_time)\n",
    "\n",
    "\n",
    "    for time in times_list:\n",
    "        data = np.loadtxt(file_path + '/probe/probe' + str(time) + '.csv', skiprows=1, delimiter=',')[..., (4, 5, 0, 1, 2,)]\n",
    "        t_len = data.shape[0]\n",
    "        supervised_t = np.array([time] * t_len).reshape((-1, 1))\n",
    "        data = np.concatenate((data[..., (0, 1)], supervised_t, data[..., (2, 3, 4)]), axis=1)\n",
    "        probe.append(data)\n",
    "\n",
    "    full_supervised_data = np.concatenate(probe)\n",
    "\n",
    "    inlet = replicate_time_list(times_list, inlet.shape[0],  inlet)\n",
    "    outlet = replicate_time_list(times_list, outlet.shape[0],  outlet)\n",
    "    initial = replicate_time_list([0.1], initial.shape[0],  initial)\n",
    "    cyl = replicate_time_list(times_list, cyl.shape[0],  cyl)\n",
    "    train = replicate_time_list(times_list, train.shape[0],  train)\n",
    "\n",
    "    return train,  inlet, outlet, cyl, full_supervised_data, initial\n",
    "\n",
    "\n",
    "def replicate_time_list(time_list, domain_shape, spatial_data):\n",
    "    all_t = []\n",
    "    count = 0\n",
    "    all_data = []\n",
    "    for t in time_list:\n",
    "        tmp_t = [t] * domain_shape\n",
    "        all_t.append(tmp_t)\n",
    "        tmp = spatial_data\n",
    "        all_data.append(tmp)\n",
    "    replicated_t = np.array(all_t).reshape(-1, 1)\n",
    "    spatial_data = np.concatenate(all_data)\n",
    "    spatial_data = np.concatenate((spatial_data[..., (0, 1)], replicated_t, spatial_data[..., (2, 3, 4)]), axis=1)\n",
    "    return spatial_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-28T03:29:10.635725Z",
     "iopub.status.busy": "2022-07-28T03:29:10.635048Z",
     "iopub.status.idle": "2022-07-28T03:29:10.660645Z",
     "shell.execute_reply": "2022-07-28T03:29:10.659954Z",
     "shell.execute_reply.started": "2022-07-28T03:29:10.635689Z"
    },
    "scrolled": true,
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "################################## 单次训练步骤  ##################################\n",
    "def train_adam(inn_var, BCs, out_true, model, Loss, optimizer,  log_loss):\n",
    "    BC_in = paddle.to_tensor(BCs[0][..., 0:3], dtype='float32', place='gpu:0')\n",
    "    BC_out = paddle.to_tensor(BCs[1][..., 0:3], dtype='float32', place='gpu:0')\n",
    "    BC_wall = paddle.to_tensor(BCs[2][..., 0:3], dtype='float32', place='gpu:0')\n",
    "    BC_initial = paddle.to_tensor(BCs[-1][..., 0:3], dtype='float32', place='gpu:0')\n",
    "    field_supervised = paddle.to_tensor(out_true[..., 0:3], dtype='float32', place='gpu:0')\n",
    "\n",
    "    BC_in_m = paddle.to_tensor(BCs[0][..., 3:], dtype='float32', place='gpu:0')\n",
    "    BC_out_m = paddle.to_tensor(BCs[1][..., 3:], dtype='float32', place='gpu:0')\n",
    "    BC_wall_m = paddle.to_tensor(BCs[2][..., 3:], dtype='float32', place='gpu:0')\n",
    "    BC_initial_m = paddle.to_tensor(BCs[-1][..., 3:], dtype='float32', place='gpu:0')\n",
    "    field_supervised_m = paddle.to_tensor(out_true[..., 3:], dtype='float32', place='gpu:0')\n",
    "\n",
    "    \n",
    "    # inn_var.requires_grad_(True)\n",
    "    # BC_in.requires_grad_(True)\n",
    "    # BC_out.requires_grad_(True)\n",
    "    # BC_wall.requires_grad_(True)\n",
    "    # BC_initial.requires_grad_(True)\n",
    "    # field_supervised.requires_grad_(True)\n",
    "    # optimizer.no_grad()\n",
    "    optimizer.clear_grad()\n",
    "    out_var = model(inn_var)\n",
    "    out_var = model.output_transform(inn_var, out_var)\n",
    "    res_i, _ = model.equation(inn_var, out_var)\n",
    "    out_var = out_var[..., 0:3]\n",
    "\n",
    "    ##inlet loss  u,v\n",
    "\n",
    "    pred_in = model(BC_in)\n",
    "    pred_in = model.output_transform(BC_in, pred_in)\n",
    "    bcs_loss_in = Loss(pred_in[..., (1, 2)], BC_in_m[..., (1, 2)])\n",
    "\n",
    "    ##outlet loss p\n",
    "    pred_out = model(BC_out)\n",
    "    pred_out = model.output_transform(BC_out, pred_out)\n",
    "    bcs_loss_out = Loss(pred_out[..., 0], BC_out_m[..., 0])\n",
    "    ##wall loss u,v\n",
    "    pred_wall = model(BC_wall)\n",
    "    pred_wall = model.output_transform(BC_wall, pred_wall)\n",
    "    bcs_loss_wall = Loss(pred_wall[..., (1, 2)], BC_wall_m[..., (1, 2)])\n",
    "    ##initial loss u v p\n",
    "    pred_initial = model(BC_initial)\n",
    "    pred_initial = model.output_transform(BC_initial, pred_initial)\n",
    "    bcs_loss_initial = Loss(pred_initial[..., (0, 1, 2)], BC_initial_m[..., (0, 1, 2)])\n",
    "\n",
    "    bcs_loss = bcs_loss_in * 5 + bcs_loss_out + bcs_loss_wall * 5 + bcs_loss_initial\n",
    "\n",
    "    ## supervised loss\n",
    "    pred_field = model(field_supervised)\n",
    "    pred_field = model.output_transform(field_supervised, pred_field)\n",
    "    supervised_loss = Loss(pred_field[..., (0, 1, 2)], field_supervised_m[..., (0, 1, 2)])\n",
    "\n",
    "    eqs_loss = (res_i ** 2).mean()\n",
    "\n",
    "    loss_batch = bcs_loss * 1. + eqs_loss + supervised_loss\n",
    "    loss_batch.backward()\n",
    "\n",
    "    # data_loss = Loss(out_var, out_true)\n",
    "    log_loss.append([eqs_loss.item(), bcs_loss.item(),\n",
    "                        bcs_loss_wall.item(), bcs_loss_in.item(),\n",
    "                        bcs_loss_out.item(), bcs_loss_initial.item(), supervised_loss.item()])\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "def train_LBFGS(inn_var, BCs, out_true, model, Loss, optimizer, scheduler, log_loss):\n",
    "    BC_in = paddle.to_tensor(BCs[0][..., 0:3], dtype='float32', place='gpu:0')\n",
    "    BC_out = paddle.to_tensor(BCs[1][..., 0:3], dtype='float32', place='gpu:0')\n",
    "    BC_wall = paddle.to_tensor(BCs[2][..., 0:3], dtype='float32', place='gpu:0')\n",
    "    BC_initial = paddle.to_tensor(BCs[-1][..., 0:3], dtype='float32', place='gpu:0')\n",
    "    field_supervised = paddle.to_tensor(out_true[..., 0:3], dtype='float32', place='gpu:0')\n",
    "\n",
    "    BC_in_m = paddle.to_tensor(BCs[0][..., 3:], dtype='float32', place='gpu:0')\n",
    "    BC_out_m =paddle.to_tensor(BCs[1][..., 3:], dtype='float32', place='gpu:0')\n",
    "    BC_wall_m = paddle.to_tensor(BCs[2][..., 3:], dtype='float32', place='gpu:0')\n",
    "    BC_initial_m = paddle.to_tensor(BCs[-1][..., 3:], dtype='float32', place='gpu:0')\n",
    "    field_supervised_m = paddle.to_tensor(out_true[..., 3:], dtype='float32', place='gpu:0')\n",
    "\n",
    "    def closure():\n",
    "        inn_var.requires_grad_(True)\n",
    "        BC_in.requires_grad_(True)\n",
    "        BC_out.requires_grad_(True)\n",
    "        BC_wall.requires_grad_(True)\n",
    "        BC_initial.requires_grad_(True)\n",
    "        field_supervised.requires_grad_(True)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out_var = model(inn_var)\n",
    "        out_var = model.output_transform(inn_var, out_var)\n",
    "        res_i, _ = model.equation(inn_var, out_var)\n",
    "        out_var = out_var[..., 0:3]\n",
    "\n",
    "        ##inlet loss  u,v\n",
    "\n",
    "        pred_in = model(BC_in)\n",
    "        pred_in = model.output_transform(BC_in, pred_in)\n",
    "        bcs_loss_in = Loss(pred_in[..., (1, 2)], BC_in_m[..., (1, 2)])\n",
    "\n",
    "        ##outlet loss p\n",
    "        pred_out = model(BC_out)\n",
    "        pred_out = model.output_transform(BC_out, pred_out)\n",
    "        bcs_loss_out = Loss(pred_out[..., 0], BC_out_m[..., 0])\n",
    "        ##wall loss u,v\n",
    "        pred_wall = model(BC_wall)\n",
    "        pred_wall = model.output_transform(BC_wall, pred_wall)\n",
    "        bcs_loss_wall = Loss(pred_wall[..., (1, 2)], BC_wall_m[..., (1, 2)])\n",
    "        ##initial loss u v p\n",
    "        pred_initial = model(BC_initial)\n",
    "        pred_initial = model.output_transform(BC_initial, pred_initial)\n",
    "        bcs_loss_initial = Loss(pred_initial[..., (0,1,2)], BC_initial_m[..., (0,1,2)])\n",
    "\n",
    "        bcs_loss = bcs_loss_in * 5 + bcs_loss_out + bcs_loss_wall * 5  + bcs_loss_initial\n",
    "\n",
    "        ## supervised loss\n",
    "        pred_field = model(field_supervised)\n",
    "        pred_field = model.output_transform(field_supervised, pred_field)\n",
    "        supervised_loss = Loss(pred_field[..., (0,1,2)], field_supervised_m[..., (0,1,2)])\n",
    "\n",
    "\n",
    "        eqs_loss = (res_i ** 2).mean()\n",
    "\n",
    "\n",
    "        loss_batch = bcs_loss * 1. + eqs_loss + supervised_loss\n",
    "        loss_batch.backward()\n",
    "\n",
    "        # data_loss = Loss(out_var, out_true)\n",
    "        log_loss.append([eqs_loss.item(), bcs_loss.item(),\n",
    "                         bcs_loss_wall.item(),  bcs_loss_in.item(),\n",
    "                         bcs_loss_out.item(), bcs_loss_initial.item(), supervised_loss.item()])\n",
    "\n",
    "        return loss_batch\n",
    "\n",
    "    optimizer.step(closure)\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "################################## 预测  ##################################\n",
    "def inference(inn_var, model):\n",
    "    inn_var = inn_var.cuda()\n",
    "    inn_var.requires_grad_(True)\n",
    "    out_var = model(inn_var)\n",
    "    out_var = model.output_transform(inn_var, out_var)\n",
    "    equation, _ = model.equation(inn_var, out_var)\n",
    "    return out_var.detach().cpu(), equation.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-07-28T03:29:15.958191Z",
     "iopub.status.busy": "2022-07-28T03:29:15.957296Z",
     "iopub.status.idle": "2022-07-28T03:29:16.004296Z",
     "shell.execute_reply": "2022-07-28T03:29:16.003175Z",
     "shell.execute_reply.started": "2022-07-28T03:29:15.958155Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "/home/aistudio/data/2D_cylinder/paddle_openfoam/domain_cylinder.csv not found.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_253/4276082510.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     20\u001B[0m     \u001B[0;31m#################### 读入数据 ####################\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 21\u001B[0;31m     \u001B[0mdata_ori\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mread_paddle_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnum_time\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     22\u001B[0m     \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrandom\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpermutation\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata_ori\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m# np.random.shuffle & random.shuffle 返回None,此外， python 3 中map返回的是迭代器\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     23\u001B[0m     \u001B[0minput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_253/3924761414.py\u001B[0m in \u001B[0;36mread_paddle_data\u001B[0;34m(num_time)\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mread_paddle_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnum_time\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     38\u001B[0m     \u001B[0mfile_path\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m'/home/aistudio/data/2D_cylinder/paddle_openfoam/'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 39\u001B[0;31m     \u001B[0mcyl\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloadtxt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfile_path\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m'domain_cylinder.csv'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mskiprows\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdelimiter\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m','\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m...\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;36m4\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m5\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;31m#xypuv\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     40\u001B[0m     \u001B[0minlet\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloadtxt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfile_path\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m'domain_inlet.csv'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mskiprows\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdelimiter\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m','\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m...\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;36m4\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m5\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     41\u001B[0m     \u001B[0moutlet\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloadtxt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfile_path\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m'domain_outlet.csv'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mskiprows\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdelimiter\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m','\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m...\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;36m4\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m5\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/numpy/lib/npyio.py\u001B[0m in \u001B[0;36mloadtxt\u001B[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows)\u001B[0m\n\u001B[1;32m    959\u001B[0m             \u001B[0mfname\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mos_fspath\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    960\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0m_is_string_like\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 961\u001B[0;31m             \u001B[0mfh\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_datasource\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'rt'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mencoding\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mencoding\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    962\u001B[0m             \u001B[0mfencoding\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfh\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'encoding'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'latin1'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    963\u001B[0m             \u001B[0mfh\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0miter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfh\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/numpy/lib/_datasource.py\u001B[0m in \u001B[0;36mopen\u001B[0;34m(path, mode, destpath, encoding, newline)\u001B[0m\n\u001B[1;32m    193\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    194\u001B[0m     \u001B[0mds\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mDataSource\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdestpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 195\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mds\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mencoding\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mencoding\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnewline\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mnewline\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    196\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    197\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/numpy/lib/_datasource.py\u001B[0m in \u001B[0;36mopen\u001B[0;34m(self, path, mode, encoding, newline)\u001B[0m\n\u001B[1;32m    533\u001B[0m                                       encoding=encoding, newline=newline)\n\u001B[1;32m    534\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 535\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mIOError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"%s not found.\"\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    536\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    537\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mOSError\u001B[0m: /home/aistudio/data/2D_cylinder/paddle_openfoam/domain_cylinder.csv not found."
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    name = 'trans-cylinder-2d-mixed-'\n",
    "    work_path = os.path.join('work', name)\n",
    "    isCreated = os.path.exists(work_path)\n",
    "    if not isCreated:\n",
    "        os.makedirs(work_path)\n",
    "\n",
    "    if paddle.fluid.is_compiled_with_cuda():\n",
    "        paddle.set_device(\"gpu\")\n",
    "    else:\n",
    "        paddle.set_device('cpu')\n",
    "        \n",
    "    #################### 定义问题相关参数 ####################\n",
    "    Rho, Miu, D = 1.0, 0.02,  2\n",
    "    num_time = 50         #####随机抽取时间步个数\n",
    "    # U_max, tmax = 0.5, 0.5  # 入口流速的最大值以及非定常周期 tmax = T/2\n",
    "    # Box = [0, 0, 1.1, 0.41]  # 矩形流域\n",
    "    \n",
    "    #################### 读入数据 ####################\n",
    "    data_ori = read_paddle_data(num_time)\n",
    "    data = list(map(np.random.permutation, data_ori)) # np.random.shuffle & random.shuffle 返回None,此外， python 3 中map返回的是迭代器\n",
    "    input = data[0]\n",
    "    input = paddle.to_tensor(input[:, :3], dtype='float32', place='gpu:0')\n",
    "    BCs = (data[1], data[2], data[3], data[5]) ## 边界数据\n",
    "    field = data[4]   ##检测的流场点\n",
    "\n",
    "\n",
    "    # 采用三角形 对非结构化网格建立节点连接关系\n",
    "    triang = matplotlib.tri.Triangulation(data[-1][:, 0], data[-1][:, 1])\n",
    "    triang.set_mask(np.hypot(data[-1][triang.triangles, 0].mean(axis=1),\n",
    "                             data[-1][triang.triangles, 1].mean(axis=1)) < D/2)\n",
    "\n",
    "    # plt.figure(1, figsize=(20, 5))\n",
    "    # t = plt.tricontourf(triang, data[-1][:, 3])\n",
    "    # plt.axis('equal')\n",
    "    # plt.show()\n",
    "\n",
    "    #################### 定义损失函数、优化器以及网络结构 ####################\n",
    "    L2Loss = nn.MSELoss()\n",
    "    Net_model = Net(planes=[3] + 5 * [50] + [5], rho=Rho, miu=Miu)\n",
    "\n",
    "    Optimizer_1 = paddle.optimizer.Adam(parameters=Net_model.parameters(), learning_rate=0.0005, beta1=0.9, beta2=0.999)\n",
    "    # Optimizer_2 = paddle.incubate.optimizer.functional.minimize_lbfgs(parameters=Net_model.parameters(), learning_rate=.1, max_iter=100)\n",
    "\n",
    "    Boundary_epoch = [200000, 250000, 300000]\n",
    "\n",
    "    # Scheduler_2 = paddle.optim.lr_scheduler.MultiStepLR(Optimizer_2, milestones=Boundary_epoch, gamma=0.1)\n",
    "\n",
    "    Visual = matplotlib_vision('/', field_name=('p', 'u', 'v'), input_name=('x', 'y'))\n",
    "\n",
    "    ################################### 训练 #####################################\n",
    "    star_time = time.time()\n",
    "    log_loss = []\n",
    "    \"\"\"load a pre-trained model\"\"\"\n",
    "\n",
    "    Net_model.loadmodel(work_path + '\\\\latest_model.pth')\n",
    "    for epoch in range(Boundary_epoch[-1]):\n",
    "\n",
    "        #如果GPU内存不充足，可以分批次进行训练\n",
    "\n",
    "        if epoch < 10000:\n",
    "\n",
    "            iter = 2\n",
    "            for i in range(iter):\n",
    "                data_itr = list(map(lambda x: x[i * int(x.shape[0] / iter):(i + 1) * int(x.shape[0] / iter)], data))\n",
    "                input = data_itr[0]\n",
    "                input = paddle.to_tensor(input[:, :3], dtype='float32', place='gpu:0')\n",
    "                BCs = (data_itr[1], data_itr[2], data_itr[3], data_itr[5])  ## 边界数据\n",
    "                field = data_itr[4]  ##检测的流场点\n",
    "                train_adam(input, BCs, field, Net_model, L2Loss, Optimizer_1, log_loss)\n",
    "            learning_rate = 0.0005\n",
    "        # if epoch >= 10000:\n",
    "        #     iter = 2\n",
    "        #     for i in range(iter):\n",
    "        #         data_itr = list(map(lambda x: x[i * int(x.shape[0] / iter):(i + 1) * int(x.shape[0] / iter)], data))\n",
    "        #         input = data_itr[0]\n",
    "        #         input = paddle.to_tensor(input[:, :3], dtype='float32', place='gpu:0')\n",
    "        #         BCs = (data_itr[1], data_itr[2], data_itr[3], data_itr[5])  ## 边界数据\n",
    "        #         field = data_itr[4]  ##检测的流场点\n",
    "        #         train_LBFGS(input, BCs, field, Net_model, L2Loss, Optimizer_2, Scheduler_2, log_loss)\n",
    "        #     learning_rate = Optimizer_2.state_dict()['param_groups'][0]['lr']\n",
    "\n",
    "        if epoch > 0 and epoch % 2000 == 0:\n",
    "            print('epoch: {:6d}, lr: {:.1e}, cost: {:.2e}, dat_loss: {:.2e}, eqs_loss: {:.2e}, bcs_loss: {:.2e}'.\n",
    "                  format(epoch, learning_rate, time.time() - star_time,\n",
    "                         log_loss[-1][-1], log_loss[-1][0], log_loss[-1][1],))\n",
    "            star_time = time.time()\n",
    "\n",
    "            # 损失曲线\n",
    "            plt.figure(1, figsize=(15, 5))\n",
    "            plt.clf()\n",
    "            Visual.plot_loss(np.arange(len(log_loss)), np.array(log_loss)[:, 0], 'eqs_loss')\n",
    "            Visual.plot_loss(np.arange(len(log_loss)), np.array(log_loss)[:, 1], 'bcs_loss')\n",
    "            Visual.plot_loss(np.arange(len(log_loss)), np.array(log_loss)[:, -1], 'dat_loss')\n",
    "            plt.savefig(os.path.join(work_path, 'log_loss.svg'))\n",
    "\n",
    "            # 详细的损失曲线\n",
    "            plt.figure(2, figsize=(15, 10))\n",
    "            plt.clf()\n",
    "            plt.subplot(211)\n",
    "            Visual.plot_loss(np.arange(len(log_loss)), np.array(log_loss)[:, 0], 'eqs_loss')\n",
    "            plt.subplot(212)\n",
    "            Visual.plot_loss(np.arange(len(log_loss)), np.array(log_loss)[:, 1], 'bcs_loss')\n",
    "            Visual.plot_loss(np.arange(len(log_loss)), np.array(log_loss)[:, 2], 'wall_loss')\n",
    "            Visual.plot_loss(np.arange(len(log_loss)), np.array(log_loss)[:, 3], 'in_loss')\n",
    "            Visual.plot_loss(np.arange(len(log_loss)), np.array(log_loss)[:, 4], 'out_loss')\n",
    "            Visual.plot_loss(np.arange(len(log_loss)), np.array(log_loss)[:, 5], 'ini_loss')\n",
    "            plt.savefig(os.path.join(work_path, 'detail_loss.svg'))\n",
    "\n",
    "\n",
    "            # 根据模型预测流场， 若有真实场，则与真实场对比\n",
    "            input_visual_p = torch.tensor(data[-1][..., :3], dtype=torch.float32)  # 取初场的空间坐标\n",
    "            input_visual_p[:, -1] = input_visual_p[:, -1]    # 时间取最大\n",
    "            field_visual_p, _ = inference(input_visual_p, Net_model)\n",
    "            field_visual_t = data[-1][..., 3:]\n",
    "            field_visual_p = field_visual_p.cpu().numpy()[..., 0:3]\n",
    "            # field_visual_t = field_visual_p\n",
    "\n",
    "            plt.figure(3, figsize=(30, 8))\n",
    "            plt.clf()\n",
    "            Visual.plot_fields_tr(field_visual_t, field_visual_p, input_visual_p.detach().cpu().numpy(), triang)\n",
    "            # plt.savefig(res_path + 'field_' + str(t) + '-' + str(epoch) + '.jpg')\n",
    "            plt.savefig(os.path.join(work_path, 'global_' + str(epoch) + '.jpg'), dpi=200)\n",
    "            plt.savefig(os.path.join(work_path, 'global_now.jpg'))\n",
    "\n",
    "\n",
    "            paddle.save({'epoch': epoch, 'model': Net_model.state_dict(), }, os.path.join(work_path, 'latest_model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}